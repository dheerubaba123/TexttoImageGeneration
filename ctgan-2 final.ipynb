{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11047766,"sourceType":"datasetVersion","datasetId":6882165},{"sourceId":11047849,"sourceType":"datasetVersion","datasetId":6882233},{"sourceId":11304121,"sourceType":"datasetVersion","datasetId":7069428},{"sourceId":11318381,"sourceType":"datasetVersion","datasetId":7079558},{"sourceId":11329624,"sourceType":"datasetVersion","datasetId":7087105},{"sourceId":11386951,"sourceType":"datasetVersion","datasetId":7130461},{"sourceId":11632154,"sourceType":"datasetVersion","datasetId":7298243},{"sourceId":11632219,"sourceType":"datasetVersion","datasetId":7298273}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom nltk.tokenize import RegexpTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:32:10.854296Z","iopub.execute_input":"2025-05-01T04:32:10.854507Z","iopub.status.idle":"2025-05-01T04:32:10.876096Z","shell.execute_reply.started":"2025-05-01T04:32:10.854492Z","shell.execute_reply":"2025-05-01T04:32:10.875520Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom torchvision.models import Inception_V3_Weights\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n    \n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   \n        192: 1,  \n        768: 2,  \n        2048: 3  \n    }\n\n    def __init__(self,\n                 output_blocks=[3],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False,\n                 use_pretrained=True):\n        super(InceptionV3, self).__init__()\n        \n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n        \n        assert self.last_needed_block <= 3, 'Last possible output block index is 3'\n        \n        # Load InceptionV3 with proper weight handling\n        weights = Inception_V3_Weights.IMAGENET1K_V1 if use_pretrained else None\n        \n        # Load model with compatible parameters\n        kwargs = {\n            'weights': weights,\n            'transform_input': False,  # We'll handle normalization ourselves\n            'aux_logits': True  # Must be True for pretrained weights\n        }\n        \n        try:\n            inception = models.inception_v3(**kwargs)\n        except Exception as e:\n            print(f\"Error loading pretrained model: {e}\")\n            inception = models.inception_v3(weights=None, aux_logits=False)\n        \n        # Remove aux logits branch if present\n        if hasattr(inception, 'AuxLogits'):\n            inception.AuxLogits = None\n        \n        # Freeze model if not training\n        if not requires_grad:\n            for param in inception.parameters():\n                param.requires_grad = False\n        \n        # Build feature extraction blocks\n        self.blocks = nn.ModuleList()\n        \n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n        \n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n        \n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n        \n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n        \n        # Store normalization parameters\n        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n    def forward(self, x):\n        \"\"\"Extract features from input tensor\"\"\"\n        features = []\n        \n        # Resize if needed\n        if self.resize_input:\n            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n        \n        # Normalize if needed\n        if self.normalize_input:\n            x = (x - self.mean.to(x.device)) / self.std.to(x.device)\n        \n        # Extract features from each block\n        for block_idx, block in enumerate(self.blocks):\n            x = block(x)\n            if block_idx in self.output_blocks:\n                features.append(x)\n            if block_idx == self.last_needed_block:\n                break\n                \n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:10:52.604487Z","iopub.execute_input":"2025-05-01T04:10:52.604674Z","iopub.status.idle":"2025-05-01T04:10:52.616309Z","shell.execute_reply.started":"2025-05-01T04:10:52.604660Z","shell.execute_reply":"2025-05-01T04:10:52.615791Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"****Configurations required****","metadata":{}},{"cell_type":"code","source":"import argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Distributed Training\n    parser.add_argument('--world_size', default=-1, type=int, help='Number of nodes for distributed training')\n    parser.add_argument('--rank', default=-1, type=int, help='Node rank for distributed training')\n    parser.add_argument('--local_rank', default=0, type=int, help='Node rank for distributed training')\n    parser.add_argument('--dist_url', default='env://', type=str, help='URL for distributed training')\n    parser.add_argument('--dist_backend', default='nccl', type=str, help='Distributed backend')\n\n    # General Settings\n    parser.add_argument('--seed', default=12345, type=int, help='Seed for initializing training')\n    parser.add_argument('--gpu', default='', type=str, help='GPU to use (leave blank for CPU only)')\n\n    # Dataset and Paths\n    parser.add_argument('--dataset', type=str, default='birds', help='Dataset type')\n    parser.add_argument('--data_path', type=str, default='/kaggle/input/dataset-cub/CUB-200-2011', help='Base path')\n    parser.add_argument('--image_dir', type=str, default='./output_birds', help='Path to save images')\n    parser.add_argument('--bbox_file', type=str, default='/kaggle/input/dataset-cub/CUB-200-2011/bounding_boxes.txt', help='Bounding box file')\n    parser.add_argument('--text_dir', type=str, default='/kaggle/input/dataset-birds/birds/birds/text_c10', help='Path to text descriptions')\n    parser.add_argument('--train_filenames', type=str, default='/kaggle/input/dataset-cub/CUB-200-2011/train/filenames.pickle', help='Train filenames pickle')\n    parser.add_argument('--test_filenames', type=str, default='/kaggle/input/dataset-cub/CUB-200-2011/test/filenames.pickle', help='Test filenames pickle')\n    parser.add_argument('--c_dim', type=int, default=128, help='Text embedding dimension')\n    parser.add_argument('--text_path', type=str, default=\"/kaggle/input/dataset-birds/birds/text_c10\",\n                        help='Path to the text_c10 folder containing raw captions')\n\n    # Training Parameters\n    parser.add_argument('--STAGE', type=int, default=1, help='Training stage')\n    parser.add_argument('--epoch', type=int, default=200, help='Number of epochs')\n    parser.add_argument('--batch_size', type=int, default=16, help='Batch size to use')\n    parser.add_argument('--gener_batch_size', type=int, default=32, help='Batch size for generator')\n    parser.add_argument('--dis_batch_size', type=int, default=32, help='Batch size for discriminator')\n    parser.add_argument('--num_workers', type=int, default=4, help='Number of CPU threads for DataLoader')\n\n    # Model Parameters\n    parser.add_argument('--image_size', type=int, default=32, help='Size of image for discriminator input')\n    parser.add_argument('--initial_size', type=int, default=8, help='Initial size for generator')\n    parser.add_argument('--patch_size', type=int, default=4, help='Patch size for generated image')\n    parser.add_argument('--num_classes', type=int, default=1, help='Number of classes for discriminator')\n    parser.add_argument('--lr_gen', type=float, default=0.0001, help='Learning rate for generator')\n    parser.add_argument('--lr_dis', type=float, default=0.0001, help='Learning rate for discriminator')\n    parser.add_argument('--weight_decay', type=float, default=1e-3, help='Weight decay')\n    parser.add_argument('--z_dim', type=int, default=128, help='Latent dimension')\n    parser.add_argument('--n_critic', type=int, default=5, help='Number of critic updates per generator update')\n    parser.add_argument('--max_iter', type=int, default=250000, help='Maximum iterations')\n\n    # Optimization\n    parser.add_argument('--optim', type=str, default=\"Adam\", help='Optimizer')\n    parser.add_argument('--loss', type=str, default=\"wgangp-mode\", help='Loss function')\n    parser.add_argument('--lr_decay', action='store_true', help='Enable learning rate decay')\n    parser.add_argument('--beta1', type=float, default=0.0, help='Beta1 for Adam optimizer')\n    parser.add_argument('--beta2', type=float, default=0.99, help='Beta2 for Adam optimizer')\n\n    # Conditioning and Validation\n    parser.add_argument('--Iscondtion', action='store_true', help='Use text conditioning')\n    parser.add_argument('--Isval', action='store_true', help='Enable validation')\n\n    # Pretrained Models\n    parser.add_argument('--NET_G', type=str, default='', help='Path to Generator weights')\n    parser.add_argument('--NET_D', type=str, default='', help='Path to Discriminator weights')\n    parser.add_argument('--STAGE1_G', type=str, default='', help='Path to Stage 1 Generator weights')\n    parser.add_argument('--STAGE1_D', type=str, default='', help='Path to Stage 1 Discriminator weights')\n\n    # Snapshot & Normalization\n    parser.add_argument('--SNAPSHOT_INTERVAL', type=int, default=5000, help='Interval for saving snapshots')\n    parser.add_argument('--g_norm', type=str, default=\"ln\", help='Generator Normalization')\n    parser.add_argument('--g_act', type=str, default=\"gelu\", help='Generator Activation Layer')\n    parser.add_argument('--d_act', type=str, default=\"gelu\", help='Discriminator Activation Layer')\n    parser.add_argument('--d_norm', type=str, default=\"ln\", help='Discriminator Normalization')\n\n    # Text Encoding\n    parser.add_argument('--RNN_TYPE', type=str, default='LSTM', help='Text encoding type')\n    parser.add_argument('--WORDS_NUM', type=int, default=18, help='Number of words in text input')\n    parser.add_argument('--CAPTIONS_PER_IMAGE', type=int, default=5, help='Captions per image')\n    parser.add_argument('--EMBEDDING_DIM', type=int, default=256, help='Embedding dimension')\n    parser.add_argument('--CONDITION_DIM', type=int, default=256, help='Condition dimension')\n    # Evaluation Paths\n    parser.add_argument('--dims', type=int, default=2048, choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),\n                        help='Dimensionality of Inception features to use.')\n\n    args, unknown = parser.parse_known_args()\n    return args\n\nargs = parse_args()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:10:56.281784Z","iopub.execute_input":"2025-05-01T04:10:56.282578Z","iopub.status.idle":"2025-05-01T04:10:56.298970Z","shell.execute_reply.started":"2025-05-01T04:10:56.282556Z","shell.execute_reply":"2025-05-01T04:10:56.298280Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\nclass RNN_ENCODER(nn.Module):\n    def __init__(self, ntoken, cfg):\n        super(RNN_ENCODER, self).__init__()\n        self.ntoken = ntoken\n        self.ninput = cfg.TEXT.EMBEDDING_DIM\n        self.drop_prob = cfg.TEXT.DROP_PROB\n        self.nhidden = cfg.TEXT.HIDDEN_DIM\n        self.nlayers = cfg.TEXT.NUM_LAYERS\n        self.bidirectional = cfg.TEXT.BIDIRECTIONAL\n        self.rnn_type = cfg.TEXT.RNN_TYPE.upper()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.define_module()\n\n    def define_module(self):\n        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n        \n        rnn_cls = nn.LSTM if self.rnn_type == 'LSTM' else nn.GRU\n        self.rnn = rnn_cls(\n            self.ninput, self.nhidden, self.nlayers, batch_first=True,\n            dropout=self.drop_prob, bidirectional=self.bidirectional\n        )\n        \n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, captions, cap_lens, hidden=None):\n        embeddings = self.encoder(captions)\n        packed = nn.utils.rnn.pack_padded_sequence(embeddings, cap_lens, batch_first=True, enforce_sorted=True)\n        output, hidden = self.rnn(packed, hidden)\n        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n\n        if self.bidirectional:\n            hidden = hidden.view(self.nlayers, 2, -1, self.nhidden)\n            hidden = hidden[:, -1].contiguous().view(self.nlayers, -1, self.nhidden * 2)\n        else:\n            hidden = hidden.view(self.nlayers, -1, self.nhidden)\n\n        return output, hidden.squeeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:05:50.818350Z","iopub.status.idle":"2025-05-01T04:05:50.818562Z","shell.execute_reply.started":"2025-05-01T04:05:50.818464Z","shell.execute_reply":"2025-05-01T04:05:50.818474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    # Filter out the invalid (empty) images in the batch\n    batch = [item for item in batch if item != torch.tensor([])]\n\n    # If the batch is empty after filtering, handle accordingly (e.g., return an empty batch)\n    if not batch:\n        return torch.tensor([]), torch.tensor([])\n\n    # Standard collate for non-empty items\n    return torch.stack([item[0] for item in batch]), torch.stack([item[1] for item in batch])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:05:56.118065Z","iopub.execute_input":"2025-05-01T04:05:56.118352Z","iopub.status.idle":"2025-05-01T04:05:56.123850Z","shell.execute_reply.started":"2025-05-01T04:05:56.118331Z","shell.execute_reply":"2025-05-01T04:05:56.122935Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.utils.data.distributed import DistributedSampler\n\nclass ImageDataset(object):\n    def __init__(self, args, cur_img_size=None, is_distributed=False):\n        self.imsize = [cur_img_size * (2 ** i) for i in range(args.STAGE)]\n        img_size = self.imsize[args.STAGE - 1]\n\n        if args.dataset.lower() == 'birds':  # CUB-200 Dataset\n            transform = transforms.Compose([\n                transforms.Resize(int(img_size * 76 / 64)),  # Resize first\n                transforms.RandomCrop(img_size),  # Then crop\n                transforms.RandomHorizontalFlip(),  # Data augmentation\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n            transform_test = transforms.Compose([\n                transforms.Resize(int(img_size * 76 / 64)),\n                transforms.CenterCrop(img_size),  # No random cropping for test\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Matching training normalization\n            ])\n\n            # Load full dataset first\n            train_dataset = BIRDS(data_dir=args.data_path, split='train', imsize=img_size, transform=transform)\n            test_dataset = BIRDS(data_dir=args.data_path, split='test', imsize=img_size, transform=transform)\n            \n            # Handling Distributed Training\n            if is_distributed:\n                train_sampler = DistributedSampler(train_dataset)\n                test_sampler = DistributedSampler(test_dataset)\n                self.train = DataLoader(\n                    train_dataset,\n                    batch_size=args.dis_batch_size,\n                    shuffle=False,  # Sampler handles shuffling\n                    num_workers=args.num_workers,\n                    pin_memory=True,\n                    drop_last=True,\n                    collate_fn=custom_collate_fn,\n                    sampler=train_sampler\n                )\n                self.test = DataLoader(\n                    test_dataset,\n                    batch_size=args.dis_batch_size,\n                    shuffle=False,\n                    num_workers=args.num_workers,\n                    pin_memory=True,\n                    drop_last=True,\n                    collate_fn=custom_collate_fn,\n                    sampler=test_sampler\n                )\n            else:\n                # Standard non-distributed DataLoader\n                self.train = DataLoader(\n                    train_dataset,\n                    batch_size=args.dis_batch_size,\n                    shuffle=True,\n                    num_workers=args.num_workers,\n                    pin_memory=True,\n                    drop_last=True,\n                    collate_fn=custom_collate_fn\n                )\n                \n                self.test = DataLoader(\n                    test_dataset,\n                    batch_size=args.dis_batch_size,\n                    shuffle=False,\n                    num_workers=args.num_workers,\n                    pin_memory=True,\n                    drop_last=True,\n                    collate_fn=custom_collate_fn\n                )\n\n        else:\n            raise NotImplementedError(f\"Dataset '{args.dataset}' is not supported yet.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:11:14.832410Z","iopub.execute_input":"2025-05-01T04:11:14.832783Z","iopub.status.idle":"2025-05-01T04:11:14.843965Z","shell.execute_reply.started":"2025-05-01T04:11:14.832726Z","shell.execute_reply":"2025-05-01T04:11:14.843057Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import os\nimport pickle\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom collections import defaultdict\nfrom nltk.tokenize import RegexpTokenizer\nfrom torch.utils.data import Dataset\n\nBASE_DIR = \"/kaggle/input/dataset-cub\"  # Change this to your dataset folder\nTRAIN_EMBEDDINGS_PATH = os.path.join(BASE_DIR, \"CUB-200-2011/train/char-CNN-RNN-embeddings.pickle\")\nTEST_EMBEDDINGS_PATH = os.path.join(BASE_DIR, \"CUB-200-2011/test/char-CNN-RNN-embeddings.pickle\")\n\n# Load configuration\nargs = cfg.parse_args()\n\ndef get_imgs(img_path, transform=None, bbox=None):\n    img = Image.open(img_path).convert('RGB')\n    if bbox is not None:\n        r = int(np.maximum(bbox[2], bbox[3]) * 0.65)\n        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n        y1 = max(0, center_y - r)\n        y2 = min(img.height, center_y + r)\n        x1 = max(0, center_x - r)\n        x2 = min(img.width, center_x + r)\n        img = img.crop([x1, y1, x2, y2])\n    return transform(img) if transform else img\n\nclass BIRDS(Dataset):\n    def __init__(self, data_dir=BASE_DIR, split='train', imsize=64, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.split = split\n        self.img_size = imsize\n        self.embeddings_num = args.CAPTIONS_PER_IMAGE\n        \n        self.filenames = self.load_filenames(split)\n        self.bbox = self.load_bbox()\n        self.sent_emb = self.load_embedding(split) if args.Iscondtion else None\n    \n    def load_filenames(self, split):\n        if split == 'train':\n            filepath = args.train_filenames  # Use train filenames file\n        elif split == 'test':\n            filepath = args.test_filenames   # Use test filenames file\n        else:\n            raise ValueError(f\"Invalid dataset split: {split}\")\n        \n        if not os.path.exists(filepath):\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n        \n        with open(filepath, 'rb') as f:\n            return pickle.load(f)\n\n    \n    def load_bbox(self):\n        bbox_path = os.path.join(self.data_dir, \"bounding_boxes.txt\")\n        df_bounding_boxes = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n        \n        filepath = os.path.join(self.data_dir, \"images.txt\")\n        df_filenames = pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        \n        return {img_file[:-4]: df_bounding_boxes.iloc[i][1:].tolist() for i, img_file in enumerate(filenames)}\n    \n    def load_embedding(self, split):\n        emb_path = TRAIN_EMBEDDINGS_PATH if split == 'train' else TEST_EMBEDDINGS_PATH\n        with open(emb_path, 'rb') as f:\n            return pickle.load(f)\n    \n    def __getitem__(self, index):\n        key = self.filenames[index]\n        bbox = self.bbox.get(key, None)\n        img_path = os.path.join(self.data_dir, f\"images/{key}.jpg\")\n        img = get_imgs(img_path, self.transform, bbox)\n    \n        if args.Iscondtion:\n            sent_ix = np.random.randint(0, self.embeddings_num)\n            sent_emb = self.sent_emb[index][sent_ix]\n            return img, sent_emb  # âœ… Always return a tuple\n        else:\n            return img, torch.tensor([])  # âœ… Return an empty tensor for text_emb if not using conditions\n\n    \n    def __len__(self):\n        return len(self.filenames)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:11:17.590505Z","iopub.execute_input":"2025-05-01T04:11:17.590800Z","iopub.status.idle":"2025-05-01T04:11:17.605529Z","shell.execute_reply.started":"2025-05-01T04:11:17.590779Z","shell.execute_reply":"2025-05-01T04:11:17.604750Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nfrom torch.utils.data import Dataset\nfrom scipy import linalg\nfrom torch.nn.functional import adaptive_avg_pool2d\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nargs = cfg.parse_args()\n\n# âœ… Ensure paths are properly set\nargs.path1 = \"/kaggle/input/dataset-cub/CUB-200-2011/images\"   # âœ… Path for real images\nargs.path2 = \"/kaggle/working/test_outputs\"   # âœ… Path for generated images\n\ndef get_activations(args, images, model, dims, device, num_img, verbose=False):\n        model.eval()\n        pred_arr = []\n    \n        with torch.no_grad():\n            for batch in images:\n                if isinstance(batch, (list, tuple)):\n                    batch = batch[0]  # In case dataset returns (img, label)\n                batch = batch.to(device)\n    \n                pred = model(batch)[0]\n                pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n    \n                activations = pred.cpu().data.numpy().reshape(batch.size(0), -1)\n                pred_arr.append(activations)\n    \n                if sum(x.shape[0] for x in pred_arr) >= num_img:\n                    break\n    \n        pred_arr = np.concatenate(pred_arr, axis=0)[:num_img]\n        return pred_arr\n\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Computes Frechet Inception Distance (FID).\"\"\"\n    mu1, mu2 = np.atleast_1d(mu1), np.atleast_1d(mu2)\n    sigma1, sigma2 = np.atleast_2d(sigma1), np.atleast_2d(sigma2)\n\n    diff = mu1 - mu2\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n\n    if not np.isfinite(covmean).all():\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean))\n\ndef calculate_activation_statistics(args, images, model, dims=2048, device='cuda', num_img=5000):\n    \"\"\"Compute activations, mean and covariance matrix.\"\"\"\n    act = get_activations(args, images, model, dims, device, num_img)\n    return np.mean(act, axis=0), np.cov(act, rowvar=False)\n\ndef calculate_fid(args, test_loader, device=None, dims=2048, num_img=5000):\n    \"\"\"Compute FID score between real and generated images.\"\"\"\n    if device:\n        device = torch.device(device)\n    else:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    print(f\"âœ… Using device: {device}\")\n\n    # âœ… Load InceptionV3 model\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n    model = InceptionV3([block_idx]).to(device)\n\n    # âœ… Load real images (test set)\n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    real_dataset = Sample_evaluate(args.path1, transform=transform)\n    real_loader = torch.utils.data.DataLoader(\n        real_dataset, batch_size=args.gener_batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=True\n    )\n\n    # âœ… Load generated images\n    gen_dataset = Sample_evaluate(args.path2, transform=transform)\n    gen_loader = torch.utils.data.DataLoader(\n        gen_dataset, batch_size=args.gener_batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=True\n    )\n\n    print(\"âœ… Computing activations for real images...\")\n    mu1, sigma1 = calculate_activation_statistics(args, real_loader, model, dims, device, num_img)\n\n    print(\"âœ… Computing activations for generated images...\")\n    mu2, sigma2 = calculate_activation_statistics(args, gen_loader, model, dims, device, num_img)\n\n    fid_value = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n    print(f\"âœ… Computed FID Score: {fid_value:.4f}\")\n\n    return fid_value\n\nclass Sample_evaluate(Dataset):\n    def __init__(self, path, transform=None):\n        \"\"\"Dataset for loading images recursively from subdirectories.\"\"\"\n        self.transform = transform\n        self.data_dir = path\n\n        # Recursively find all image paths\n        self.image_paths = []\n        for root, _, files in os.walk(self.data_dir):\n            for file in files:\n                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                    self.image_paths.append(os.path.join(root, file))\n\n        if not self.image_paths:\n            raise RuntimeError(f\"No valid image files found in directory: {self.data_dir}\")\n\n    def get_imgs(self, img_path, transform=None, bbox=None):\n        try:\n            img = Image.open(img_path).convert('RGB')\n\n            if bbox is not None:\n                r = int(np.maximum(bbox[2], bbox[3]) * 0.65)\n                center_x = int((2 * bbox[0] + bbox[2]) / 2)\n                center_y = int((2 * bbox[1] + bbox[3]) / 2)\n                y1 = max(0, center_y - r)\n                y2 = min(img.height, center_y + r)\n                x1 = max(0, center_x - r)\n                x2 = min(img.width, center_x + r)\n                img = img.crop([x1, y1, x2, y2])\n\n            return transform(img) if transform else img\n\n        except Exception as e:\n            print(f\"âš ï¸ Warning: Failed to load image at '{img_path}' due to: {e}\")\n            return None\n\n    def __getitem__(self, index):\n        for _ in range(len(self.image_paths)):\n            img_path = self.image_paths[index]\n            img = self.get_imgs(img_path, self.transform)\n            if img is not None:\n                return img\n            else:\n                index = (index + 1) % len(self.image_paths)\n        raise RuntimeError(\"âŒ All images in dataset are invalid or failed to load.\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n\n# âœ… Run FID calculation\nif __name__ == \"__main__\":\n    fid_score = calculate_fid(args, test_loader=None, device=\"cuda\", dims=2048, num_img=5000)\n    print(f\"ðŸ”¥ Final FID Score: {fid_score:.4f}\")","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-05-01T04:17:58.715935Z","iopub.execute_input":"2025-05-01T04:17:58.716226Z","iopub.status.idle":"2025-05-01T04:18:38.111652Z","shell.execute_reply.started":"2025-05-01T04:17:58.716204Z","shell.execute_reply":"2025-05-01T04:18:38.110688Z"}},"outputs":[{"name":"stdout","text":"âœ… Using device: cuda\nâœ… Computing activations for real images...\nâœ… Computing activations for generated images...\nâœ… Computed FID Score: 153.7975\nðŸ”¥ Final FID Score: 153.7975\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_images(dataset, n=5):\n    plt.figure(figsize=(15,3))\n    for i in range(n):\n        img = dataset[i].permute(1,2,0).numpy()\n        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]) # Unnormalize\n        plt.subplot(1,n,i+1)\n        plt.imshow(np.clip(img,0,1))\n        plt.axis('off')\n\nshow_images(real_dataset)  # Your real images\nshow_images(gen_dataset)   # Your generated images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport warnings\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torchvision.models import inception_v3\nfrom scipy.stats import entropy\n\nwarnings.filterwarnings(\"ignore\")\n\n# Define arguments manually (since Kaggle doesn't support config.cfg)\nclass Args:\n    def __init__(self):\n        self.gener_batch_size = 32  # Change as per available memory\n\nargs = Args()\n\n# Inception Score Calculation\ndef inception_score(imgs, Evaluate_loader, cuda=True, batch_size=32, resize=False, splits=1):\n    \"\"\"Computes the Inception Score (IS) of the generated images\"\"\"\n    N = len(imgs)\n    assert batch_size > 0 and N > batch_size\n\n    device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")\n\n    # Load Inception model\n    inception_model = inception_v3(weights='IMAGENET1K_V1', transform_input=False).to(device)\n    inception_model.eval()\n\n    up = nn.Upsample(size=(299, 299), mode='bilinear').to(device)\n    \n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        return F.softmax(x, dim=1).detach().cpu().numpy()\n\n    preds = np.zeros((N, 1000))\n\n    for i, batch in enumerate(Evaluate_loader):\n        batch = batch.to(device)\n        batchv = Variable(batch)\n        batch_size_i = batch.size(0)\n        preds[i * batch_size: i * batch_size + batch_size_i] = get_pred(batchv)\n\n    # Compute mean KL-divergence\n    split_scores = []\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = [entropy(pyx, py) for pyx in part]\n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores)\n\n# Custom Dataset\nclass Sample_evaluate(Dataset):\n    def __init__(self, path, transform=None):\n        self.transform = transform\n        self.data_dir = path\n        self.images_names = os.listdir(self.data_dir)\n\n    def get_imgs(self, img_path, transform=None):\n        img = Image.open(img_path).convert('RGB')\n        if transform:\n            img = transform(img)\n        return img\n\n    def __getitem__(self, index):\n        key = self.images_names[index]\n        img_name = os.path.join(self.data_dir, key)\n        return self.get_imgs(img_name, self.transform)\n\n    def __len__(self):\n        return len(self.images_names)\n\n# Ignore Label Dataset\nclass IgnoreLabelDataset(Dataset):\n    def __init__(self, orig):\n        self.orig = orig\n\n    def __getitem__(self, index):\n        return self.orig[index]\n\n    def __len__(self):\n        return len(self.orig)\n\n# Function to calculate IS\ndef calculate_IS(path):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    dataset = Sample_evaluate(path, transform=transform)\n    Evaluate_loader = DataLoader(dataset, batch_size=args.gener_batch_size, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n    \n    print(\"Calculating Inception Score...\")\n    IS_score = inception_score(IgnoreLabelDataset(dataset), Evaluate_loader, cuda=True, batch_size=args.gener_batch_size, resize=True, splits=10)\n    \n    return IS_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.315448Z","iopub.execute_input":"2025-05-01T04:06:23.315685Z","iopub.status.idle":"2025-05-01T04:06:23.328202Z","shell.execute_reply.started":"2025-05-01T04:06:23.315667Z","shell.execute_reply":"2025-05-01T04:06:23.327555Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import numpy as np\nimport warnings\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torchvision.models import inception_v3, Inception_V3_Weights\nfrom scipy.stats import entropy\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Args:\n    def __init__(self):\n        self.gener_batch_size = 32  # Adjust based on available memory\n\nargs = Args()\n\nclass SampleDataset(Dataset):\n    def __init__(self, path, transform=None):\n        self.transform = transform or transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        self.image_paths = [os.path.join(path, f) for f in os.listdir(path) \n                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        \n    def __getitem__(self, index):\n        img = Image.open(self.image_paths[index]).convert('RGB')\n        return self.transform(img)\n    \n    def __len__(self):\n        return len(self.image_paths)\n\ndef inception_score(dataset, batch_size=32, splits=10, cuda=True):\n    \"\"\"Computes Inception Score for a given dataset\"\"\"\n    N = len(dataset)\n    device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")\n    \n    # Load inception model\n    model = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, \n                        transform_input=False).to(device)\n    model.eval()\n    \n    # Create dataloader\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n    \n    # Get predictions\n    preds = []\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = batch.to(device)\n            outputs = model(batch)\n            preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n    \n    preds = np.concatenate(preds, axis=0)\n    \n    # Compute IS\n    scores = []\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores.append(np.exp(np.mean([entropy(p, py) for p in part])))\n    \n    return np.mean(scores), np.std(scores)\n\ndef calculate_inception_score(image_dir):\n    transform = transforms.Compose([\n        transforms.Resize(299),\n        transforms.CenterCrop(299),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    dataset = SampleDataset(image_dir, transform=transform)\n    mean, std = inception_score(dataset, batch_size=args.gener_batch_size)\n    print(f\"Inception Score: {mean:.2f} Â± {std:.2f}\")\n    return mean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.340150Z","iopub.execute_input":"2025-05-01T04:06:23.340566Z","iopub.status.idle":"2025-05-01T04:06:23.351459Z","shell.execute_reply.started":"2025-05-01T04:06:23.340544Z","shell.execute_reply":"2025-05-01T04:06:23.350923Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models\nfrom torch.autograd import Variable\nimport torch.utils.data\nfrom torchvision.models.inception import inception_v3\nimport numpy as np\nfrom scipy.stats import entropy\nargs = cfg.parse_args()\n\ntry:\n    from torchvision.models.utils import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\n\n\nFID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'  # noqa: E501\n\n\nclass InceptionV3(nn.Module):\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=(DEFAULT_BLOCK_INDEX,),\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False,\n                 use_fid_inception=True):\n     \n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        if use_fid_inception:\n            inception = fid_inception_v3()\n        else:\n            inception = _inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n       \n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.interpolate(x,\n                              size=(299, 299),\n                              mode='bilinear',\n                              align_corners=False)\n\n        if self.normalize_input:\n            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp\n\n\ndef fid_inception_v3():\n    inception = models.inception_v3(num_classes=1008,\n                              aux_logits=False,\n                              pretrained=False)\n    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n    inception.Mixed_7b = FIDInceptionE_1(1280)\n    inception.Mixed_7c = FIDInceptionE_2(2048)\n\n    #state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n    state_dict = \\\n        torch.load(args.inception,map_location=lambda storage, loc: storage)\n    inception.load_state_dict(state_dict)\n    return inception\n\nclass FIDInceptionA(torchvision.models.inception.InceptionA):\n    \"\"\"InceptionA block patched for FID computation\"\"\"\n    def __init__(self, in_channels, pool_features):\n        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        # Patch: Tensorflow's average pool does not use the padded zero's in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass FIDInceptionC(torchvision.models.inception.InceptionC):\n    \"\"\"InceptionC block patched for FID computation\"\"\"\n    def __init__(self, in_channels, channels_7x7):\n        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        # Patch: Tensorflow's average pool does not use the padded zero's in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass FIDInceptionE_1(torchvision.models.inception.InceptionE):\n    \"\"\"First InceptionE block patched for FID computation\"\"\"\n    def __init__(self, in_channels):\n        super(FIDInceptionE_1, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        # Patch: Tensorflow's average pool does not use the padded zero's in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass FIDInceptionE_2(torchvision.models.inception.InceptionE):\n    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n    def __init__(self, in_channels):\n        super(FIDInceptionE_2, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        # Patch: The FID Inception model uses max pooling instead of average\n        # pooling. This is likely an error in this specific Inception\n        # implementation, as other Inception models use average pooling here\n        # (which matches the description in the paper).\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\ndef inception_score(imgs,Evaluate_loader, cuda=True, batch_size=32, resize=False, splits=1):\n    \"\"\"Computes the inception score of the generated images imgs\n    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n    cuda -- whether or not to run on GPU\n    batch_size -- batch size for feeding into Inception v3\n    splits -- number of splits\n    \"\"\"\n    N = len(imgs)\n\n    assert batch_size > 0\n    assert N > batch_size\n\n    # Set up dtype\n    if cuda:\n        dtype = torch.cuda.FloatTensor\n    else:\n        if torch.cuda.is_available():\n            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n        dtype = torch.FloatTensor\n\n    # Set up dataloader\n    dataloader = Evaluate_loader\n\n    # Load inception model\n    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n    inception_model.eval();\n    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        preds = F.softmax(x, dim=1).data.cpu().numpy()\n        # Ensure no NaN or Inf values\n        if np.any(np.isnan(preds)) or np.any(np.isinf(preds)):\n            print(\"Warning: NaN or Inf in prediction!\")\n            preds = np.zeros_like(preds)\n        return preds\n\n\n    # Get predictions\n    preds = np.zeros((N, 1000))\n\n    for i, batch in enumerate(dataloader):\n        batch = batch.type(dtype)\n        batchv = Variable(batch)\n        batch_size_i = batch.size()[0]\n\n        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n\n    # Now compute the mean kl-div\n    split_scores = []\n\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = []\n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            scores.append(entropy(pyx, py))\n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores), np.std(split_scores)\n\n\nif __name__ == '__main__':\n    class IgnoreLabelDataset(torch.utils.data.Dataset):\n        def __init__(self, orig):\n            self.orig = orig\n\n        def __getitem__(self, index):\n            return self.orig[index][0]\n\n        def __len__(self):\n            return len(self.orig)\n\n    import torchvision.datasets as dset\n    import torchvision.transforms as transforms\n\n\n    transform = transforms.Compose([\n        transforms.Resize((128,128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    data_dir = \"/kaggle/working/test_outputs\"\n    dataset = Sample_evaluate(data_dir, transform=transform)  # Pass the correct path to the images\n\n    # Create DataLoader for evaluation\n    Evaluate_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=8, shuffle=True, num_workers=0, pin_memory=True, drop_last=True\n    )\n    \n    # Wrap dataset for Inception Score computation\n    IgnoreLabelDataset(dataset)\n    \n    # Compute Inception Score\n    print(\"Calculating Inception Score...\")\n    print(inception_score(IgnoreLabelDataset(dataset), Evaluate_loader, cuda=True, batch_size=8, resize=True, splits=10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.580778Z","iopub.execute_input":"2025-05-01T04:06:23.581203Z","iopub.status.idle":"2025-05-01T04:06:23.589965Z","shell.execute_reply.started":"2025-05-01T04:06:23.581186Z","shell.execute_reply":"2025-05-01T04:06:23.589248Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom scipy.stats import entropy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import models, transforms\nfrom torchvision.models.inception import inception_v3\nfrom PIL import Image\n\n# Configuration class\nclass Args:\n    def __init__(self):\n        self.gener_batch_size = 32\n        self.inception = None  # Path to FID weights if needed\n\nargs = Args()\n\n# Dataset class that handles subdirectories\nclass Sample_evaluate(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        self.image_paths = []\n        \n        # Recursively find all images in subdirectories\n        for dirpath, _, filenames in os.walk(root_dir):\n            for filename in filenames:\n                if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n                    self.image_paths.append(os.path.join(dirpath, filename))\n        \n    def __getitem__(self, index):\n        try:\n            img = Image.open(self.image_paths[index]).convert('RGB')\n            if self.transform is not None:\n                img = self.transform(img)\n            return img\n        except Exception as e:\n            print(f\"Error loading image {self.image_paths[index]}: {e}\")\n            # Return blank image if loading fails\n            return torch.zeros(3, 299, 299)\n        \n    def __len__(self):\n        return len(self.image_paths)\n\n# Dataset wrapper to ignore labels\nclass IgnoreLabelDataset(Dataset):\n    def __init__(self, orig):\n        self.orig = orig\n\n    def __getitem__(self, index):\n        return self.orig[index]\n\n    def __len__(self):\n        return len(self.orig)\n\n# FID-specific InceptionV3 implementation\nclass FIDInceptionA(torchvision.models.inception.InceptionA):\n    \"\"\"InceptionA block patched for FID computation\"\"\"\n    def __init__(self, in_channels, pool_features):\n        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                 count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass FIDInceptionC(torchvision.models.inception.InceptionC):\n    \"\"\"InceptionC block patched for FID computation\"\"\"\n    def __init__(self, in_channels, channels_7x7):\n        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                 count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass FIDInceptionE_1(torchvision.models.inception.InceptionE):\n    \"\"\"First InceptionE block patched for FID computation\"\"\"\n    def __init__(self, in_channels):\n        super(FIDInceptionE_1, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                 count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\nclass FIDInceptionE_2(torchvision.models.inception.InceptionE):\n    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n    def __init__(self, in_channels):\n        super(FIDInceptionE_2, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\ndef fid_inception_v3():\n    \"\"\"Build pretrained Inception model for FID computation\"\"\"\n    inception = models.inception_v3(num_classes=1008,\n                                  aux_logits=False,\n                                  pretrained=False)\n    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n    inception.Mixed_7b = FIDInceptionE_1(1280)\n    inception.Mixed_7c = FIDInceptionE_2(2048)\n\n    if args.inception:\n        state_dict = torch.load(args.inception, \n                              map_location=lambda storage, loc: storage)\n    else:\n        # Load default pretrained weights\n        state_dict = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1).state_dict()\n    \n    inception.load_state_dict(state_dict)\n    return inception\n\ndef inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=10):\n    \"\"\"Computes the inception score of the generated images imgs\n    \n    Args:\n        imgs: Torch dataset of (3xHxW) numpy images normalized in [-1, 1]\n        cuda: whether or not to run on GPU\n        batch_size: batch size for feeding into Inception v3\n        splits: number of splits\n    Returns:\n        Mean and std of inception score\n    \"\"\"\n    N = len(imgs)\n    \n    if N < batch_size:\n        print(f\"Warning: Increasing batch size from {batch_size} to {N} because not enough samples\")\n        batch_size = N\n\n    # Set up device\n    device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Load inception model\n    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n    inception_model.eval()\n    \n    # Upsampler if needed\n    up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=False).to(device)\n\n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        return F.softmax(x, dim=1).data.cpu().numpy()\n\n    # Get predictions\n    preds = np.zeros((N, 1000))\n    dataloader = DataLoader(imgs, batch_size=batch_size)\n\n    for i, batch in enumerate(dataloader):\n        batch = batch.to(device)\n        batch_size_i = batch.size()[0]\n\n        with torch.no_grad():\n            preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batch)\n\n    # Now compute the mean kl-div\n    split_scores = []\n    eps = 1e-16  # Small constant for numerical stability\n\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = []\n        \n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            # Add epsilon to avoid log(0)\n            pyx = np.maximum(pyx, eps)\n            py = np.maximum(py, eps)\n            scores.append(entropy(pyx, py))\n        \n        # Check for invalid values before exponentiation\n        if len(scores) > 0:\n            mean_score = np.mean(scores)\n            if not np.isnan(mean_score) and not np.isinf(mean_score):\n                split_scores.append(np.exp(mean_score))\n            else:\n                print(f\"Warning: Invalid score in split {k}\")\n\n    if not split_scores:  # If all splits had issues\n        return float('nan'), float('nan')\n    \n    return np.mean(split_scores), np.std(split_scores)\n\nif __name__ == '__main__':\n    # Set up transforms - must match Inception's expected input\n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),  # Inception expects 299x299\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Path to your images (with subdirectories for classes)\n    data_dir = \"/kaggle/working/test_outputs\"\n    \n    # Create dataset\n    dataset = Sample_evaluate(data_dir, transform=transform)\n    \n    # Wrap dataset to ignore labels\n    ignore_label_dataset = IgnoreLabelDataset(dataset)\n    \n    # Check if we have enough images\n    if len(ignore_label_dataset) == 0:\n        print(\"Error: No images found in directory\")\n    else:\n        print(f\"Found {len(ignore_label_dataset)} images\")\n        \n        # Compute Inception Score\n        print(\"Calculating Inception Score...\")\n        mean, std = inception_score(\n            ignore_label_dataset,\n            cuda=True,\n            batch_size=args.gener_batch_size,\n            resize=False,  # We already resized to 299x299\n            splits=10\n        )\n        \n        print(f\"Inception Score: {mean:.2f} Â± {std:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.753868Z","iopub.execute_input":"2025-05-01T04:06:23.754049Z","iopub.status.idle":"2025-05-01T04:06:23.782115Z","shell.execute_reply.started":"2025-05-01T04:06:23.754036Z","shell.execute_reply":"2025-05-01T04:06:23.781526Z"}},"outputs":[{"name":"stdout","text":"Error: No images found in directory\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.models import Inception_V3_Weights\nfrom scipy.stats import entropy\nfrom PIL import Image\n\nclass Args:\n    def __init__(self):\n        self.gener_batch_size = 32\n        self.inception = None  # Path to FID weights if needed\n\nargs = Args()\n\nclass Sample_evaluate(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        self.image_paths = []\n        \n        # Walk through all subdirectories to find images\n        for subdir, _, files in os.walk(root_dir):\n            for file in files:\n                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                    self.image_paths.append(os.path.join(subdir, file))\n        \n    def __getitem__(self, index):\n        try:\n            img = Image.open(self.image_paths[index]).convert('RGB')\n            return self.transform(img)\n        except Exception as e:\n            print(f\"Error loading image {self.image_paths[index]}: {e}\")\n            # Return blank image if loading fails\n            return torch.zeros(3, 299, 299)\n        \n    def __len__(self):\n        return len(self.image_paths)\n\nclass IgnoreLabelDataset(Dataset):\n    def __init__(self, orig):\n        self.orig = orig\n\n    def __getitem__(self, index):\n        return self.orig[index]\n\n    def __len__(self):\n        return len(self.orig)\n\ndef inception_score(dataset, batch_size=32, splits=10, cuda=True):\n    \"\"\"Compute Inception Score for a dataset\"\"\"\n    N = len(dataset)\n    device = torch.device(\"cuda\" if cuda and torch.cuda.is_available() else \"cpu\")\n    \n    # Load inception model\n    model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1,\n                               transform_input=False).to(device)\n    model.eval()\n    \n    # Get predictions\n    dataloader = DataLoader(dataset, batch_size=batch_size)\n    preds = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            batch = batch.to(device)\n            outputs = model(batch)\n            probs = F.softmax(outputs, dim=1)\n            preds.append(probs.cpu().numpy())\n    \n    preds = np.concatenate(preds, axis=0)\n    \n    # Calculate IS with numerical stability\n    scores = []\n    eps = 1e-16  # Small constant to avoid log(0)\n    \n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0) + eps\n        part = part + eps\n        \n        # Calculate KL divergence\n        kl = part * (np.log(part) - np.log(py))\n        kl = np.sum(kl, axis=1)\n        scores.append(np.exp(np.mean(kl)))\n    \n    return np.mean(scores), np.std(scores)\n\ndef calculate_inception_score(image_dir):\n    \"\"\"Main function to calculate Inception Score\"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    try:\n        dataset = Sample_evaluate(image_dir, transform=transform)\n        if len(dataset) == 0:\n            raise ValueError(\"No valid images found in directory\")\n        \n        # Wrap dataset to ignore labels (if any)\n        dataset = IgnoreLabelDataset(dataset)\n        \n        mean, std = inception_score(\n            dataset,\n            batch_size=args.gener_batch_size,\n            splits=10,\n            cuda=True\n        )\n        print(f\"Inception Score: {mean:.2f} Â± {std:.2f}\")\n        return mean\n        \n    except Exception as e:\n        print(f\"Error calculating Inception Score: {e}\")\n        return float('nan')\n\nif __name__ == \"__main__\":\n    # Point to the root directory containing class subdirectories\n    image_dir = \"/kaggle/input/dataset-cub/CUB-200-2011/images\"  \n    score = calculate_inception_score(image_dir)\n    print(f\"Final Inception Score: {score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.794293Z","iopub.execute_input":"2025-05-01T04:06:23.794980Z","iopub.status.idle":"2025-05-01T04:06:23.806490Z","shell.execute_reply.started":"2025-05-01T04:06:23.794957Z","shell.execute_reply":"2025-05-01T04:06:23.805747Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\ndef compute_discriminator_loss(args, netD, real_imgs, fake_imgs, conditions=None, dis_mode='full'):\n    device = real_imgs.device\n    criterion = nn.BCELoss()\n    batch_size = real_imgs.size(0)\n\n    real_label = torch.full((batch_size,), 1., dtype=torch.float, device=device)\n    fake_label = torch.full((batch_size,), 0., dtype=torch.float, device=device)\n\n    if getattr(args, \"Iscondtion\", False):  # Ensure args.Iscondtion exists\n        # Real pairs\n        real_logits = netD(real_imgs, conditions, dis_mode).view(-1)\n        errD_real = criterion(real_logits, real_label)\n\n        # Wrong pairs (shifted conditions)\n        wrong_logits = netD(real_imgs[:batch_size - 1], conditions[1:]).view(-1)\n        errD_wrong = criterion(wrong_logits, fake_label[1:])\n\n        # Fake pairs\n        fake_logits = netD(fake_imgs, conditions, dis_mode).view(-1)\n        errD_fake = criterion(fake_logits, fake_label)\n\n        errD = errD_real + (errD_fake + errD_wrong) * 0.5\n        return errD, errD_real, errD_wrong, errD_fake\n    else:\n        real_logits = netD(real_imgs, dis_mode).view(-1)\n        errD_real = criterion(real_logits, real_label)\n\n        fake_logits = netD(fake_imgs, dis_mode).view(-1)\n        errD_fake = criterion(fake_logits, fake_label)\n\n        errD = errD_real + errD_fake\n        return errD, errD_real, errD_fake\n\ndef compute_generator_loss(args, netD, fake_imgs, conditions=None, dis_mode='full'):\n    device = fake_imgs.device\n    real_label = torch.full((fake_imgs.shape[0],), 1., dtype=torch.float, device=device)\n    criterion = nn.BCELoss()\n\n    if getattr(args, \"Iscondtion\", False):  # Conditional\n        cond = conditions.detach()\n        fake_logits = netD(fake_imgs, cond, dis_mode).view(-1)\n    else:\n        fake_logits = netD(fake_imgs, dis_mode).view(-1)\n\n    errG = criterion(fake_logits, real_label)\n    return errG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.807565Z","iopub.execute_input":"2025-05-01T04:06:23.807743Z","iopub.status.idle":"2025-05-01T04:06:23.831056Z","shell.execute_reply.started":"2025-05-01T04:06:23.807729Z","shell.execute_reply":"2025-05-01T04:06:23.830516Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport math\nimport warnings\nfrom itertools import repeat\nfrom collections.abc import Iterable  # Replaces deprecated torch._six\n\n# DropPath (Stochastic Depth)\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (for residual blocks).\"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # Work with different tensor dims\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # Binarize\n    return x.div(keep_prob) * random_tensor\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) for residual blocks.\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n# Function to handle different tuple sizes\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\n\n# Truncated Normal Distribution\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    \"\"\"Fills tensor with values from a truncated normal distribution.\"\"\"\n    def norm_cdf(x):\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"Mean is more than 2 std from [a, b]. The distribution may be incorrect.\", stacklevel=2)\n\n    with torch.no_grad():\n        l, u = norm_cdf((a - mean) / std), norm_cdf((b - mean) / std)\n        tensor.uniform_(2 * l - 1, 2 * u - 1).erfinv_()\n        tensor.mul_(std * math.sqrt(2.)).add_(mean)\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    \"\"\"Truncated normal distribution initialization.\"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\n# Example usage (Colab-ready)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx = torch.randn(10, 10).to(device)\ndrop_layer = DropPath(0.5).to(device)\n\nprint(\"Original Tensor:\\n\", x)\nprint(\"After DropPath:\\n\", drop_layer(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:23.832231Z","iopub.execute_input":"2025-05-01T04:06:23.832566Z","iopub.status.idle":"2025-05-01T04:06:24.998091Z","shell.execute_reply.started":"2025-05-01T04:06:23.832550Z","shell.execute_reply":"2025-05-01T04:06:24.997409Z"}},"outputs":[{"name":"stdout","text":"Original Tensor:\n tensor([[-0.1655,  0.2034,  0.7755,  1.2473,  0.5719,  0.8826,  0.7003, -0.3900,\n          0.7372, -2.3263],\n        [ 0.4776, -0.6503,  0.9010, -0.5001, -1.0648, -1.3095, -0.0838,  0.0549,\n          0.3487, -1.1700],\n        [-0.1974,  0.7314, -1.0342,  1.2728,  0.4853, -1.4157,  0.8394, -0.5128,\n         -0.3210, -0.5561],\n        [ 0.5385, -1.6113, -1.0079,  1.1051, -0.1755, -0.2883,  1.6926, -1.1681,\n         -0.5770,  1.1492],\n        [-0.0103,  0.6808, -0.4248,  0.4011, -0.7257, -0.1550, -0.2028, -0.3032,\n         -0.6097, -0.4261],\n        [-1.7541, -0.3349,  0.4381,  0.4129, -0.0070,  1.7640,  1.5498,  0.1098,\n          0.0137, -0.1646],\n        [-0.1620,  0.2261,  0.3850,  0.3997, -0.4631,  0.6551, -0.4844,  1.0146,\n         -0.6451, -0.0915],\n        [-0.3202,  1.7779, -0.7696,  0.0527,  0.8011, -0.7848,  0.8314,  1.6104,\n          0.8489, -0.3973],\n        [-1.4742,  0.2968, -1.5962,  0.9027,  0.7020,  0.8075,  0.7069, -0.2810,\n          1.8428,  0.6330],\n        [-0.4221,  0.1344,  0.9174, -0.1286,  0.2964,  1.0539,  0.5309, -0.7719,\n         -0.5884, -1.2078]], device='cuda:0')\nAfter DropPath:\n tensor([[-0.3311,  0.4068,  1.5511,  2.4946,  1.1438,  1.7651,  1.4006, -0.7800,\n          1.4745, -4.6525],\n        [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n          0.0000, -0.0000],\n        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n         -0.0000, -0.0000],\n        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n         -0.0000,  0.0000],\n        [-0.0206,  1.3617, -0.8496,  0.8022, -1.4515, -0.3101, -0.4057, -0.6064,\n         -1.2194, -0.8523],\n        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000, -0.0000],\n        [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n         -0.0000, -0.0000],\n        [-0.6404,  3.5558, -1.5392,  0.1054,  1.6021, -1.5696,  1.6627,  3.2208,\n          1.6978, -0.7945],\n        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n          0.0000,  0.0000],\n        [-0.8442,  0.2688,  1.8349, -0.2572,  0.5927,  2.1077,  1.0618, -1.5439,\n         -1.1768, -2.4155]], device='cuda:0')\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nfrom pdb import set_trace as stx\n\n\ndef UpSampling(x, H, W):\n    B, N, C = x.size()\n    \n    # Check if the number of channels is divisible by 4 (as PixelShuffle requires this)\n    assert C % 4 == 0, \"Number of channels must be divisible by 4 for PixelShuffle\"\n    \n    # Check if the input size matches the expected height and width\n    assert N == H * W, \"The number of tokens (N) must match the height * width (H * W)\"\n    \n    # Permute and reshape the tensor to match PixelShuffle input format\n    x = x.permute(0, 2, 1).view(-1, C, H, W)\n    \n    # Apply PixelShuffle for upsampling\n    x = nn.PixelShuffle(2)(x)  # This upsamples spatial dimensions by a factor of 2\n    \n    # Get the new height and width\n    _, C, H, W = x.size()  # Update the H, W after PixelShuffle\n\n    # Reshape and permute the tensor back to match the original format\n    x = x.view(-1, C, H * W).permute(0, 2, 1)\n    \n    return x, H, W\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = CustomAct(act_layer)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass matmul(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        x = x1 @ x2\n        return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:25.895597Z","iopub.execute_input":"2025-05-01T04:06:25.896136Z","iopub.status.idle":"2025-05-01T04:06:25.912087Z","shell.execute_reply.started":"2025-05-01T04:06:25.896113Z","shell.execute_reply":"2025-05-01T04:06:25.911500Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.mat = matmul()\n        self.window_size = window_size\n        if self.window_size != 0:\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size)\n            coords_w = torch.arange(window_size)\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size - 1\n            relative_coords[:, :, 0] *= 2 * window_size - 1\n            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n        if self.window_size != 0:\n            relative_position_bias = self.relative_position_bias_table[\n                self.relative_position_index.view(-1).clone()].view(\n                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:26.318888Z","iopub.execute_input":"2025-05-01T04:06:26.319110Z","iopub.status.idle":"2025-05-01T04:06:26.328369Z","shell.execute_reply.started":"2025-05-01T04:06:26.319093Z","shell.execute_reply":"2025-05-01T04:06:26.327692Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, window_size=16):\n        super().__init__()\n        self.norm1 = CustomNorm(norm_layer, dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n            window_size=window_size)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = CustomNorm(norm_layer, dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass SwinBlock(nn.Module):\n    def __init__(self, dim, input_resolution, num_heads, window_size=16, shift_size=8,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            # Unpack the returned tuple\n            mask_windows, B, N = window_partition(img_mask, self.window_size)\n            \n            # Apply view operation only on the tensor\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n        # cyclic shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n        # partition windows\n        x_windows, B, N = window_partition(shifted_x, self.window_size)  # âœ… Unpack the tuple correctly\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # âœ… Now works correctly\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        # FFN\n        x = shortcut + self.drop_path(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:26.575421Z","iopub.execute_input":"2025-05-01T04:06:26.575859Z","iopub.status.idle":"2025-05-01T04:06:26.589869Z","shell.execute_reply.started":"2025-05-01T04:06:26.575839Z","shell.execute_reply":"2025-05-01T04:06:26.589205Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, depth, dim, heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, window_size=16):\n        super().__init__()\n        self.depth = depth\n        models = [Block(\n            dim=dim,\n            num_heads=heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop=drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            window_size=window_size\n        ) for i in range(depth)]\n        self.block = nn.Sequential(*models)\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass SwinTransformerEncoder(nn.Module):\n    def __init__(self, depth, dim, input_resolution, heads=4, window_size=16, shift_size=8, mlp_ratio=4.,\n                 qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.depth = depth\n        models = [SwinBlock(\n            dim=dim,\n            input_resolution=input_resolution,\n            num_heads=heads,\n            window_size=window_size,\n            shift_size=0 if (i % 2 == 0) else window_size // 2,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop=drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        ) for i in range(depth)]\n        self.block = nn.Sequential(*models)\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\ndef bicubic_upsample(x, H, W):\n    B, N, C = x.size()\n    assert N == H * W\n    x = x.permute(0, 2, 1)\n    x = x.view(-1, C, H, W)\n    x = nn.functional.interpolate(x, scale_factor=2, mode='bicubic')\n    B, C, H, W = x.size()\n    x = x.view(-1, C, H * W)\n    x = x.permute(0, 2, 1)\n    return x, H, W\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    x = x.permute(0, 2, 3, 1)  # Convert shape to (B, H, W, C)\n    B, H, W, C = x.shape\n\n    # âœ… Ensure H and W are valid for partitioning\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n\n    x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h), mode=\"constant\", value=0)  # Pad with zeros\n    H, W = x.shape[1], x.shape[2]  # Update new H and W\n\n    # âœ… Ensure valid shape\n    assert H > 0 and W > 0, f\"Invalid H={H}, W={W} after padding\"\n    assert H % window_size == 0 and W % window_size == 0, \"H or W is not divisible by window_size\"\n\n    # âœ… Partition the window\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    x = x.permute(0, 1, 3, 5, 2, 4).contiguous().view(-1, C, window_size, window_size)\n\n    return x, B, H // window_size  # Return the partitioned tensor and dimensions\n\n\ndef window_reverse(x, window_size, H, W):\n    \"\"\"\n    Args:\n        x: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of original image\n        W (int): Width of original image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    BNN, C, H_W, W_W = x.shape  # Extract batch info\n\n    N = H // window_size  # Number of windows per row/column\n\n    # âœ… Correct reshaping\n    x = x.view(BNN // (N * N), N, N, C, window_size, window_size)\n    x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n    x = x.view(-1, C, H, W)  # Restore original shape\n\n    return x\n\n\n\nclass CustomNorm(nn.Module):\n    def __init__(self, norm_layer, dim):\n        super().__init__()\n        self.norm_type = norm_layer\n        self.norm = nn.LayerNorm(dim)\n    def forward(self, x):\n        return self.norm(x)\n\nclass CustomAct(nn.Module):\n    def __init__(self, act_layer):\n        super().__init__()\n        self.act_layer = nn.GELU()\n    def forward(self, x):\n        return self.act_layer(x)\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:26.664802Z","iopub.execute_input":"2025-05-01T04:06:26.665138Z","iopub.status.idle":"2025-05-01T04:06:26.684451Z","shell.execute_reply.started":"2025-05-01T04:06:26.665094Z","shell.execute_reply":"2025-05-01T04:06:26.683597Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\nclass Generator64(nn.Module):\n\n    def __init__(self, args, initial_size=8, dim=1024, heads=4, mlp_ratio=4, drop_rate=0.,\n                 window_size=16,depth=[5,4,4,4]):\n        super(Generator64, self).__init__()\n\n        self.initial_size = initial_size\n        self.dim = dim\n        self.args = args\n        self.window_size = window_size\n        self.c_dim = args.CONDITION_DIM\n        self.z_dim = args.z_dim\n        self.heads = heads\n        self.mlp_ratio = mlp_ratio\n        self.droprate_rate = drop_rate\n     \n        if args.Iscondtion:\n            self.mlp = nn.Linear(self.c_dim + self.z_dim, (self.initial_size ** 2) * self.dim)\n        else:\n            self.mlp = nn.Linear(self.z_dim, (self.initial_size ** 2) * self.dim)\n\n        self.positional_embedding_1 = nn.Parameter(torch.zeros(1, (self.initial_size ** 2), self.dim))\n        self.positional_embedding_2 = nn.Parameter(torch.zeros(1, (self.initial_size * 2) ** 2, self.dim // 4))\n        self.positional_embedding_3 = nn.Parameter(torch.zeros(1, (self.initial_size * 4) ** 2, self.dim // 16))\n        self.positional_embedding_4 = nn.Parameter(torch.zeros(1, 16, self.window_size ** 2, self.dim // 64))\n\n        self.TransformerEncoder_encoder1 = TransformerEncoder(depth[0], dim=self.dim, heads=self.heads,\n                                                              mlp_ratio=self.mlp_ratio, qkv_bias=False,\n                                                              qk_scale=None, drop=drop_rate, attn_drop=0.,\n                                                              drop_path=0., act_layer=args.g_act,\n                                                              norm_layer=args.g_norm,\n                                                              window_size=8)\n        self.TransformerEncoder_encoder2 = TransformerEncoder(depth[1], dim=self.dim // 4, heads=self.heads,\n                                                              mlp_ratio=self.mlp_ratio, qkv_bias=False,\n                                                              qk_scale=None, drop=drop_rate, attn_drop=0.,\n                                                              drop_path=0., act_layer=args.g_act,\n                                                              norm_layer=args.g_norm,\n                                                              window_size=16)\n        self.TransformerEncoder_encoder3 = TransformerEncoder(depth[2], dim=self.dim // 16, heads=self.heads,\n                                                              mlp_ratio=self.mlp_ratio, qkv_bias=False,\n                                                              qk_scale=None, drop=drop_rate, attn_drop=0.,\n                                                              drop_path=0., act_layer=args.g_act,\n                                                              norm_layer=args.g_norm,\n                                                              window_size=32)\n        self.TransformerEncoder_encoder4 = SwinTransformerEncoder(depth[3], input_resolution=(64, 64),\n                                                              dim=self.dim // 64, heads=self.heads,\n                                                              window_size=self.window_size,\n                                                              shift_size=self.window_size//2,\n                                                              mlp_ratio=4., qkv_bias=False,\n                                                              qk_scale=None, drop=0., attn_drop=0.,\n                                                              drop_path=0., act_layer=nn.GELU,\n                                                              norm_layer=nn.LayerNorm)\n        self.norm = nn.LayerNorm(16)\n        self.linear = nn.Sequential(nn.Conv2d(self.dim // 64, 3, 1, 1, 0))\n        \n    def forward(self, z_code, sent_emb=None, output_dir=\"output\", step=0):\n        if self.args.Iscondtion:\n            c_z_code = torch.cat((z_code, sent_emb), 1)\n        else:\n            c_z_code = z_code\n\n        x = self.mlp(c_z_code).view(-1, self.initial_size ** 2, self.dim)\n        x = x + self.positional_embedding_1\n        H, W = self.initial_size, self.initial_size\n        x = self.TransformerEncoder_encoder1(x)\n\n    \n\n        x, H, W = UpSampling(x, H, W)\n        x = x + self.positional_embedding_2\n        x = self.TransformerEncoder_encoder2(x)\n\n    \n\n        x, H, W = UpSampling(x, H, W)\n        x = x + self.positional_embedding_3\n        x = self.TransformerEncoder_encoder3(x)\n\n\n        x, H, W = UpSampling(x, H, W)\n        x = self.TransformerEncoder_encoder4(x)\n        x = self.norm(x)\n        x = self.linear(x.permute(0, 2, 1).view(-1, self.dim // 64, H, W))\n\n        return x\n\n\nclass Generator128(nn.Module):\n    def __init__(self, args, dim=1024, heads=4, mlp_ratio=4, H=16, W=16, drop_rate=0.,depth=[5,4,4,4]):\n        super(Generator128, self).__init__()\n        self.conv_dim = 256\n        self.CONDITION_DIM = args.CONDITION_DIM\n        self.args = args\n        self.window_size = 16\n        self.shift_size = self.window_size // 2\n        self.dim = dim\n        self.H = H\n        self.W = W\n        self.heads = heads\n        self.mlp_ratio = mlp_ratio\n        self.droprate_rate = drop_rate\n        self.positional_embedding_0 = nn.Parameter(torch.zeros(1, (4 * 4) ** 2, self.dim))\n        self.positional_embedding_1 = nn.Parameter(torch.zeros(1, (4 * 8) ** 2, self.dim // 4))\n        self.positional_embedding_2 = nn.Parameter(torch.zeros(1, 16, self.window_size ** 2, self.dim // 16))\n        self.positional_embedding_3 = nn.Parameter(torch.zeros(1, (16*8) ** 2, self.dim // 64))\n\n        self.TransformerEncoder_encoder0 = TransformerEncoder(depth[0], dim=self.dim, heads=self.heads,\n                                                              mlp_ratio=4., qkv_bias=False,\n                                                              qk_scale=None, drop=0., attn_drop=0.,\n                                                              drop_path=0., act_layer=nn.GELU,\n                                                              norm_layer=nn.LayerNorm,\n                                                              window_size=16)\n\n        self.TransformerEncoder_encoder1 = TransformerEncoder(depth[1], dim=self.dim // 4, heads=self.heads,\n                                                              mlp_ratio=4., qkv_bias=False,\n                                                              qk_scale=None, drop=0., attn_drop=0.,\n                                                              drop_path=0., act_layer=nn.GELU,\n                                                              norm_layer=nn.LayerNorm,\n                                                              window_size=32)\n\n        self.TransformerEncoder_encoder2 = SwinTransformerEncoder(depth[2], input_resolution=(64, 64),\n                                                                  dim=self.dim // 16, heads=self.heads,\n                                                                  window_size=self.window_size,\n                                                                  shift_size=self.shift_size,\n                                                                  mlp_ratio=4., qkv_bias=False,\n                                                                  qk_scale=None, drop=0., attn_drop=0.,\n                                                                  drop_path=0., act_layer=nn.GELU,\n                                                                  norm_layer=nn.LayerNorm)\n\n        self.TransformerEncoder_encoder3 = SwinTransformerEncoder(depth[3], input_resolution=(128, 128),\n                                                                  dim=self.dim // 64, heads=self.heads,\n                                                                  window_size=self.window_size,\n                                                                  shift_size=self.shift_size,\n                                                                  mlp_ratio=4., qkv_bias=False,\n                                                                  qk_scale=None, drop=0., attn_drop=0.,\n                                                                  drop_path=0., act_layer=nn.GELU,\n                                                                  norm_layer=nn.LayerNorm\n                                                                  )\n\n        self.deconv = nn.Sequential(nn.Conv2d(16, 3, 1, 1, 0))\n        self.norm = nn.LayerNorm(16)\n\n        self.encoder = nn.Sequential(\n            conv3x3(3, self.conv_dim),\n            nn.GELU(),\n            nn.Conv2d(self.conv_dim, self.conv_dim * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.conv_dim * 2),\n            nn.GELU(),\n            nn.Conv2d(self.conv_dim * 2, self.conv_dim * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(self.conv_dim * 4),\n            nn.GELU(),\n        )\n        self.hr_joint = nn.Sequential(\n            conv3x3(self.CONDITION_DIM + self.conv_dim * 4, self.conv_dim * 4),\n            nn.BatchNorm2d(self.conv_dim * 4),\n            nn.GELU())\n\n    def cat(self, fake_img_feature, sent_emb):\n        c_code = sent_emb.view(-1, self.CONDITION_DIM, 1, 1)\n        c_code = c_code.repeat(1, 1, fake_img_feature.shape[2], fake_img_feature.shape[2])\n        i_c_code = torch.cat([fake_img_feature, c_code], 1)\n        return i_c_code\n\n    def forward(self, stageI_images, sent_emb=None):\n\n        if self.args.Iscondtion:\n            \n            fake_img_feature = self.encoder(stageI_images)\n            i_c_code = self.cat(fake_img_feature, sent_emb)\n            fake_img1_feature = self.hr_joint(i_c_code)\n        else:\n            fake_img1_feature = self.encoder(stageI_images)\n\n        x = fake_img1_feature.view(-1, self.dim, self.H * self.W).permute(0, 2, 1)\n        x = x + self.positional_embedding_0\n        x = self.TransformerEncoder_encoder0(x)\n\n        x, H, W = UpSampling(x, self.H, self.W)\n        x = x + self.positional_embedding_1\n        B, _, C = x.size()\n        x = self.TransformerEncoder_encoder1(x)\n\n        x, H, W = UpSampling(x, H, W)\n        B, _, C = x.size()\n        #x = x + self.positional_embedding_2\n        x = self.TransformerEncoder_encoder2(x)\n\n        x, H, W = UpSampling(x, H, W)\n        B, _, C = x.size()\n        #x = x + self.positional_embedding_3\n        x = self.TransformerEncoder_encoder3(x)\n        x=self.norm(x)\n        x = x.permute(0, 2, 1).view(-1, C, H, W)\n        fake_image = self.deconv(x)\n        \n        return fake_image\n\nclass Generator256(nn.Module):\n    def __init__(self, args, dim=1024, heads=4, mlp_ratio=4, H=32, W=32, drop_rate=0., depth=[5,4,4,4,4]):\n        super(Generator256, self).__init__()\n        self.args = args\n        self.dim = dim\n        self.H = H\n        self.W = W\n        self.CONDITION_DIM = args.CONDITION_DIM if hasattr(args, 'CONDITION_DIM') else 128\n        self.z_dim = args.z_dim if hasattr(args, 'z_dim') else 100\n        self.Iscondtion = args.Iscondtion if hasattr(args, 'Iscondtion') else True\n        \n        # Initialize all layers from your checkpoint\n        self.positional_embedding_1 = nn.Parameter(torch.zeros(1, H*W, dim))\n        self.positional_embedding_2 = nn.Parameter(torch.zeros(1, (H*2)*(W*2), dim//4))\n        self.positional_embedding_3 = nn.Parameter(torch.zeros(1, (H*4)*(W*4), dim//16))\n        self.positional_embedding_4 = nn.Parameter(torch.zeros(1, (H*8)*(W*8), dim//64))\n        \n        # MLP layer to project z_code + sent_emb to initial dimensions\n        if self.Iscondtion:\n            self.mlp = nn.Linear(self.z_dim + self.CONDITION_DIM, (H * W) * dim)\n        else:\n            self.mlp = nn.Linear(self.z_dim, (H * W) * dim)\n        \n        # Transformer blocks\n        self.TransformerEncoder_encoder1 = TransformerEncoder(depth[0], dim=dim, heads=heads,\n                                                             mlp_ratio=mlp_ratio, drop_rate=drop_rate)\n        self.TransformerEncoder_encoder2 = TransformerEncoder(depth[1], dim=dim//4, heads=heads,\n                                                             mlp_ratio=mlp_ratio, drop_rate=drop_rate)\n        self.TransformerEncoder_encoder3 = SwinTransformerEncoder(depth[2], dim=dim//16, heads=heads,\n                                                                window_size=16, drop_rate=drop_rate)\n        self.TransformerEncoder_encoder4 = SwinTransformerEncoder(depth[3], dim=dim//64, heads=heads,\n                                                                window_size=16, drop_rate=drop_rate)\n        \n        # Final layers\n        self.norm = nn.LayerNorm(dim//64)\n        self.linear = nn.Sequential(\n            nn.Conv2d(dim//64, 3, kernel_size=1, stride=1, padding=0)\n        )\n\n    def forward(self, z_code, sent_emb=None):\n        if self.Iscondtion:\n            assert sent_emb is not None, \"Conditional generation requires sent_emb\"\n            z = torch.cat([z_code, sent_emb], dim=1)\n        else:\n            z = z_code\n            \n        # Project to initial dimensions\n        x = self.mlp(z).view(-1, self.H*self.W, self.dim)\n        x = x + self.positional_embedding_1\n        \n        # Transformer blocks with upsampling\n        x = self.TransformerEncoder_encoder1(x)\n        x, H, W = UpSampling(x, self.H, self.W)\n        x = x + self.positional_embedding_2\n        \n        x = self.TransformerEncoder_encoder2(x)\n        x, H, W = UpSampling(x, H, W)\n        x = x + self.positional_embedding_3\n        \n        x = self.TransformerEncoder_encoder3(x)\n        x, H, W = UpSampling(x, H, W)\n        x = x + self.positional_embedding_4\n        \n        x = self.TransformerEncoder_encoder4(x)\n        \n        # Final processing\n        x = self.norm(x)\n        x = x.permute(0, 2, 1).view(-1, self.dim//64, H, W)\n        x = self.linear(x)\n        \n        return torch.tanh(x)  # Normalize to [-1, 1]\n\n# Update your generate_images_from_text function\ndef generate_images_from_text(generator, text_descriptions, text_encoder, vocab, device, save_dir, n_images=1, z_dim=100):\n    generator.eval()\n    text_encoder.eval()\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for i, text in enumerate(text_descriptions):\n        print(f\"\\nGenerating image for: '{text}'\")\n        \n        with torch.no_grad():\n            # Encode text\n            sent_emb = encode_text(text, text_encoder, device, vocab)\n            \n            # Generate random noise\n            z_code = torch.randn(1, z_dim).to(device)\n            \n            # Generate multiple images\n            for j in range(n_images):\n                fake_imgs = generator(z_code, sent_emb)\n                \n                # Save image\n                save_path = os.path.join(save_dir, f\"text2img_{i}_{j}.png\")\n                save_image(fake_imgs, save_path, normalize=True, range=(-1, 1))\n                print(f\"âœ… Saved: {save_path}\")\n                \n                # Display image\n                display(Image.open(save_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:27.828857Z","iopub.execute_input":"2025-05-01T04:06:27.829526Z","iopub.status.idle":"2025-05-01T04:06:27.864135Z","shell.execute_reply.started":"2025-05-01T04:06:27.829502Z","shell.execute_reply":"2025-05-01T04:06:27.863593Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass FilterModule(nn.Module):\n    def __init__(self, generator, discriminator, num_candidates=5):\n        super(FilterModule, self).__init__()\n        self.generator = generator  # G0\n        self.discriminator = discriminator  # D0\n        self.num_candidates = num_candidates  # Number of images to generate\n\n    def forward(self, text_embeddings, noise):\n        batch_size = text_embeddings.size(0)\n\n        # Efficiently expand text embeddings and noise for candidates\n        text_embeddings_expanded = text_embeddings.unsqueeze(1).expand(-1, self.num_candidates, -1).contiguous()\n        noise_expanded = noise.unsqueeze(1).expand(-1, self.num_candidates, -1).contiguous()\n\n        # Flatten the expanded embeddings and noise\n        text_embeddings_expanded = text_embeddings_expanded.view(-1, text_embeddings.size(1))\n        noise_expanded = noise_expanded.view(-1, noise.size(1))\n\n        # Generate candidate images\n        candidate_images = self.generator(noise_expanded, text_embeddings_expanded)  # (batch_size * num_candidates, C, H, W)\n\n        # Get scores for the candidate images\n        scores = self.discriminator(candidate_images, text_embeddings_expanded)  # (batch_size * num_candidates, 1)\n\n        # Reshape scores and candidate images for easier manipulation\n        scores = scores.view(batch_size, self.num_candidates)  # (batch_size, num_candidates)\n        candidate_images = candidate_images.view(batch_size, self.num_candidates, *candidate_images.shape[1:])\n\n        # Select the best image based on the highest score\n        best_indices = scores.argmax(dim=1)  # Best index for each batch\n        best_images = candidate_images.gather(1, best_indices.view(-1, 1, 1).expand(-1, -1, candidate_images.size(2), candidate_images.size(3)))\n\n        return best_images  # Shape: (batch_size, C, H, W)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:28.256402Z","iopub.execute_input":"2025-05-01T04:06:28.256656Z","iopub.status.idle":"2025-05-01T04:06:28.263269Z","shell.execute_reply.started":"2025-05-01T04:06:28.256638Z","shell.execute_reply":"2025-05-01T04:06:28.262583Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pdb import set_trace as stx\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    x = x.permute(0, 2, 3, 1)  # Convert shape to (B, H, W, C)\n    B, H, W, C = x.shape\n\n    # âœ… Ensure H and W are valid for partitioning\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n\n    x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h), mode=\"constant\", value=0)  # Pad with zeros\n    H, W = x.shape[1], x.shape[2]  # Update new H and W\n\n    # âœ… Ensure valid shape\n    assert H > 0 and W > 0, f\"Invalid H={H}, W={W} after padding\"\n    assert H % window_size == 0 and W % window_size == 0, \"H or W is not divisible by window_size\"\n\n    # âœ… Partition the window\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    x = x.permute(0, 1, 3, 5, 2, 4).contiguous().view(-1, C, window_size, window_size)\n\n    return x, B, H // window_size  # Return the partitioned tensor and dimensions\n\n\ndef window_reverse(x, window_size, H, W):\n    \"\"\"\n    Args:\n        x: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of original image\n        W (int): Width of original image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    BNN, C, H_W, W_W = x.shape  # Extract batch info\n\n    N = H // window_size  # Number of windows per row/column\n\n    # âœ… Correct reshaping\n    x = x.view(BNN // (N * N), N, N, C, window_size, window_size)\n    x = x.permute(0, 3, 1, 4, 2, 5).contiguous()\n    x = x.view(-1, C, H, W)  # Restore original shape\n\n    return x\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\ndef Block3x3_leakRelu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\ndef downBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\ndef encode_image_by_16times(ndf):\n    encode_img = nn.Sequential(\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 2),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\ndef encode_image_by_16times2(ndf):\n    encode_img = nn.Sequential(\n        # nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        # nn.BatchNorm2d(ndf * 2),\n        # nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:28.264622Z","iopub.execute_input":"2025-05-01T04:06:28.264894Z","iopub.status.idle":"2025-05-01T04:06:28.288630Z","shell.execute_reply.started":"2025-05-01T04:06:28.264871Z","shell.execute_reply":"2025-05-01T04:06:28.288118Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\nclass Discriminator64(nn.Module):\n    def __init__(self,args):\n        super(Discriminator64, self).__init__()\n        self.df_dim = 64\n        self.args=args\n        self.ef_dim = args.CONDITION_DIM\n        ndf, nef = self.df_dim, self.ef_dim\n        self.window_size=16\n        self.fixed_size=32\n        self.conv1=nn.Sequential(nn.Conv2d(3, ndf, 4, 2, 1, bias=False),nn.LeakyReLU(0.2, inplace=True))\n        self.encode_img = encode_image_by_16times(ndf)\n        self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n        self.outlogits = nn.Sequential(\n            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n            nn.Sigmoid())\n\n    def sent_process(self,img_embedding ,cond):\n        cond = cond.view(-1, self.ef_dim, 1, 1)\n        cond = cond.repeat(1, 1, 4, 4)\n        h_c_code = torch.cat((img_embedding, cond), 1)\n        return h_c_code\n\n    def forward(self,image,cond=None,mode='full'):\n        \n        if mode == 'full':\n            x_code = self.conv1(image)\n        else:\n            x_code,B,N=window_partition(image,self.window_size)\n            x_code = self.conv1(x_code)\n            x_code=window_reverse(x_code,self.fixed_size,B,N)\n\n        img_embedding = self.encode_img(x_code)\n        if self.args.Iscondtion:\n            img_embedding=self.sent_process(img_embedding,cond)\n            h_c_code=self.jointConv(img_embedding)\n        else:\n            h_c_code=img_embedding\n        out=self.outlogits(h_c_code)\n        return out\n\n\nclass Discriminator128(nn.Module):\n    def __init__(self,args):\n        super(Discriminator128, self).__init__()\n        self.df_dim = 64\n        self.ef_dim = args.CONDITION_DIM\n        ndf, nef = self.df_dim, self.ef_dim\n        self.args=args\n        self.window_size=16\n        self.fixed_size=32\n        self.conv1=nn.Sequential(nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n                                nn.LeakyReLU(0.2, inplace=True),\n                                nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n                                nn.BatchNorm2d(ndf * 2),\n                                nn.LeakyReLU(0.2, inplace=True))\n        self.img_code_s16 = encode_image_by_16times2(ndf)\n        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n        self.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n        self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n        self.outlogits = nn.Sequential(\n            nn.Conv2d(ndf * 8, 1, kernel_size=3, stride=1, padding=1),\n            nn.AdaptiveAvgPool2d((1, 1)),  # ðŸ”¥ Ensures final output is (1,1)\n            nn.Sigmoid()\n        )\n\n\n\n    def sent_process(self, img_embedding, cond):\n        cond = cond.view(-1, self.ef_dim, 1, 1)\n        cond = cond.repeat(1, 1, 4, 4)\n        h_c_code = torch.cat((img_embedding, cond), 1)\n        return h_c_code\n\n    def forward(self,image,cond=None,mode='full'):\n        if mode=='full':\n            x_code = self.conv1(image)\n        else:\n            x_code,B,N=window_partition(image,self.window_size)\n            x_code = self.conv1(x_code)\n            x_code=window_reverse(x_code,self.fixed_size,B,N)\n\n        x_code = self.img_code_s16(x_code)\n        x_code = self.img_code_s32(x_code)\n        x_code = self.img_code_s32_1(x_code)\n        if self.args.Iscondtion:\n            img_embedding = self.sent_process(x_code, cond)\n            h_c_code = self.jointConv(img_embedding)\n        else:\n            h_c_code=x_code\n\n        out=self.outlogits(h_c_code)\n\n        return out\n\nclass Discriminator256(nn.Module):\n    def __init__(self, args):\n        super(Discriminator256, self).__init__()\n        self.df_dim = 64\n        self.ef_dim = args.CONDITION_DIM\n        self.args = args\n        ndf, nef = self.df_dim, self.ef_dim\n        self.window_size = 16\n        self.fixed_size = 32\n\n        # Initial convolution layers\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 256 -> 128\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),  # 128 -> 64\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),  # 64 -> 32\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        # Further downsampling to 16x16, 8x8, 4x4\n        self.img_code_s16 = downBlock(ndf * 4, ndf * 8)   # 32 -> 16\n        self.img_code_s8 = downBlock(ndf * 8, ndf * 16)   # 16 -> 8\n        self.img_code_s4 = downBlock(ndf * 16, ndf * 32)  # 8 -> 4\n        self.refine = Block3x3_leakRelu(ndf * 32, ndf * 16)  # keep depth for jointConv\n\n        # Joint conditioning\n        self.jointConv = Block3x3_leakRelu(ndf * 16 + nef, ndf * 8)\n\n        self.outlogits = nn.Sequential(\n            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),  # 4x4 -> 1x1\n            nn.Sigmoid()\n        )\n\n    def sent_process(self, img_embedding, cond):\n        cond = cond.view(-1, self.ef_dim, 1, 1)\n        cond = cond.repeat(1, 1, 4, 4)  # Match feature spatial size\n        h_c_code = torch.cat((img_embedding, cond), 1)\n        return h_c_code\n\n    def forward(self, image, cond=None, mode='full'):\n        if mode == 'full':\n            x_code = self.conv1(image)\n        else:\n            x_code, B, N = window_partition(image, self.window_size)\n            x_code = self.conv1(x_code)\n            x_code = window_reverse(x_code, self.fixed_size, B, N)\n\n        x_code = self.img_code_s16(x_code)\n        x_code = self.img_code_s8(x_code)\n        x_code = self.img_code_s4(x_code)\n        x_code = self.refine(x_code)\n\n        if self.args.Iscondtion:\n            x_code = self.sent_process(x_code, cond)\n            h_c_code = self.jointConv(x_code)\n        else:\n            h_c_code = x_code\n\n        out = self.outlogits(h_c_code)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:28.289412Z","iopub.execute_input":"2025-05-01T04:06:28.289638Z","iopub.status.idle":"2025-05-01T04:06:28.320574Z","shell.execute_reply.started":"2025-05-01T04:06:28.289623Z","shell.execute_reply":"2025-05-01T04:06:28.320108Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Perceptual Loss (using VGG features)\nclass PerceptualLoss(nn.Module):\n    def __init__(self, model):\n        super(PerceptualLoss, self).__init__()\n        self.model = model\n        self.criterion = nn.MSELoss()\n\n    def forward(self, generated, target):\n        generated_features = self.model(generated)\n        target_features = self.model(target)\n        return self.criterion(generated_features, target_features)\n\n# WGAN-GP Loss for the Discriminator\ndef wgan_gp_loss(D_real, D_fake, real_images, fake_images, lambda_gp=10):\n    # Compute gradient penalty\n    epsilon = torch.rand(real_images.size(0), 1, 1, 1).to(real_images.device)\n    interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n    interpolates.requires_grad_(True)\n    \n    D_interpolates = D(interpolates)\n    gradients = torch.autograd.grad(outputs=D_interpolates, inputs=interpolates,\n                                   grad_outputs=torch.ones(D_interpolates.size()).to(real_images.device),\n                                   create_graph=True, retain_graph=True)[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    \n    # WGAN-GP loss\n    loss = -D_real.mean() + D_fake.mean() + lambda_gp * gradient_penalty\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:28.428691Z","iopub.execute_input":"2025-05-01T04:06:28.429113Z","iopub.status.idle":"2025-05-01T04:06:28.435294Z","shell.execute_reply.started":"2025-05-01T04:06:28.429097Z","shell.execute_reply":"2025-05-01T04:06:28.434479Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"batch_size = 8\nimg_size = 64  # Change to 128 for Discriminator128\nchannels = 3\ncondition_dim = 128\n\n# Initialize models\nD64 = Discriminator64(args)  # âœ… Fixed\nD128 = Discriminator128(args)  # âœ… Fixed\n\n\n# Create random fake images and text embeddings\nfake_images = torch.randn(batch_size, channels, img_size, img_size)  # (B, 3, H, W)\ntext_embeddings = torch.randn(batch_size, condition_dim)  # (B, 128)\n\n# Forward pass\noutput_64 = D64(fake_images, text_embeddings)  # Shape: (B, 1, 1, 1)\noutput_128 = D128(fake_images, text_embeddings)  # Shape: (B, 1, 1, 1)\n\nprint(f\"D64 Output Shape: {output_64.shape}\")  # Expected: (B, 1, 1, 1)\nprint(f\"D128 Output Shape: {output_128.shape}\")  # Expected: (B, 1, 1, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:28.436515Z","iopub.execute_input":"2025-05-01T04:06:28.436935Z","iopub.status.idle":"2025-05-01T04:06:28.475091Z","shell.execute_reply.started":"2025-05-01T04:06:28.436919Z","shell.execute_reply":"2025-05-01T04:06:28.474174Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1498403601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Initialize models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mD64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# âœ… Fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mD128\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator128\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# âœ… Fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2245656634.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mef_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONDITION_DIM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mndf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mef_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Args' object has no attribute 'CONDITION_DIM'"],"ename":"AttributeError","evalue":"'Args' object has no attribute 'CONDITION_DIM'","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\nfrom torchvision.utils import  save_image\nimport os\nimport numpy as np\n\n\n\ndef noise(n_samples, z_dim, device):\n    return torch.randn(n_samples, z_dim).to(device)\n\n\nclass LinearLrDecay(object):\n    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n\n        assert start_lr > end_lr\n        self.optimizer = optimizer\n        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n        self.decay_start_step = decay_start_step\n        self.decay_end_step = decay_end_step\n        self.start_lr = start_lr\n        self.end_lr = end_lr\n    def step(self, current_step):\n        if current_step <= self.decay_start_step:\n            lr = self.start_lr\n        elif current_step >= self.decay_end_step:\n            lr = self.end_lr\n        else:\n            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = lr\n        return lr\n\n\ndef inits_weight(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_uniform_(m.weight.data, 1.)\n\n\ndef noise(imgs, latent_dim):\n    return torch.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n\n\ndef gener_noise(gener_batch_size, latent_dim):\n    return torch.FloatTensor(np.random.normal(0, 1, (gener_batch_size, latent_dim)))\n\n\ndef save_checkpoint(states, is_best, output_dir,\n                    filename='checkpoint.pth'):\n    torch.save(states, os.path.join(output_dir, filename))\n    if is_best:\n        torch.save(states, os.path.join(output_dir, 'checkpoint_best.pth'))\n\ndef save_model(netG, netD, stage, epoch, model_dir, dataset_name, FID_value, IS_value):\n    torch.save(\n        netG.state_dict(),\n        '%s/model/%s_stage_%d_netG_epoch_%d_fid_%2f_is_%2f.pth' % (\n        model_dir, dataset_name, stage, epoch, FID_value, IS_value))\n    torch.save(\n        netD.state_dict(),\n        '%s/model/%s_stage_%d_netD_epoch_last.pth' % (model_dir, dataset_name, stage))\n    print('Save G/D models')\n\n\ndef pre_gen_imgs(test_emb,pre_generator, pre_discriminator,device='cuda:0'):\n    expend_size = 10\n    fake_img_list = []\n    test_emb = test_emb.to(device)\n    for i in range(test_emb.shape[0]):\n        test_emb_expend = test_emb[i].unsqueeze(0).repeat((expend_size, 1))\n        noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (expend_size, args.z_dim)))\n        with torch.no_grad():\n            fake_img = pre_generator(noise, test_emb_expend)\n        # save_images(fake_imgs)\n        score = pre_discriminator(fake_img, test_emb_expend)\n        index = torch.argmax(score)\n        fake_img_list.append(fake_img[index])\n    fake_imgs = torch.stack(fake_img_list, 0)\n    return fake_imgs\n\ndef mk_img(args,generator,test_loader,pre_generator=None,pre_discriminator=None,num_img=30000,batch_size=args.gener_batch_size,device='cuda:0'):\n    with torch.no_grad():\n        if args.STAGE==1:\n            label=True\n            id=0\n            generator = generator.eval()\n            fp=os.path.join(args.image_dir,'evaluate_images')\n            if(not os.path.exists(fp)):\n                os.mkdir(fp)\n            print('sampling images...')\n            while label:\n                for index,(_,test_emb) in enumerate(test_loader):\n                    noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (batch_size, args.z_dim)))\n                    if args.Iscondtion :\n                        test_emb=test_emb.to(device)\n                        gen_imgs= generator(noise, test_emb)\n                    else:\n                        gen_imgs = generator(noise)\n                    for i in range(gen_imgs.shape[0]):\n                        save_name = '%s/stage-%d-%d.png' % (fp, args.STAGE, id)\n                        save_image(gen_imgs[i], save_name, nrow=1, normalize=True, scale_each=True)\n                        id += 1\n                        if id==num_img:\n                            print('finished')\n                            return fp\n\n        else:\n            id = 0\n            print('sampling images...')\n            fp = os.path.join(args.image_dir, 'evaluate_images')\n            if(not os.path.exists(fp)):\n                os.mkdir(fp)\n            for index ,(_,test_emb) in enumerate(test_loader) :\n                print(index)\n                if args.Iscondtion:\n                    test_emb = test_emb.to(device)\n                    stageI_imgs = pre_gen_imgs(test_emb,pre_generator, pre_discriminator,device=device)\n                    gen_imgs = generator(stageI_imgs, test_emb)\n                else:\n                    stageI_imgs = pre_gen_imgs(test_emb,pre_generator, pre_discriminator,device=device)\n                    gen_imgs = generator(stageI_imgs)\n                for i in range(gen_imgs.shape[0]):\n                    save_name = '%s/stage-%d-%d.png' % (fp, args.STAGE, id)\n                    save_image(gen_imgs[i], save_name, nrow=1, normalize=True, scale_each=True)\n                    id += 1\n                    if id == num_img:\n                        print('finished')\n                        return fp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:06:28.695463Z","iopub.execute_input":"2025-05-01T04:06:28.695670Z","iopub.status.idle":"2025-05-01T04:06:28.713197Z","shell.execute_reply.started":"2025-05-01T04:06:28.695653Z","shell.execute_reply":"2025-05-01T04:06:28.712588Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from IPython import display\nfrom PIL import Image\nimport torch\nimport os\nimport warnings\nfrom torchvision.utils import save_image\n\ndef show_image(image_path):\n    img = Image.open(image_path)\n    display.display(img)\n\nwarnings.filterwarnings(\"ignore\")\n\n# Load Configuration\nargs = cfg.parse_args()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using Device:\", device)\n\nos.makedirs(\"/kaggle/working/models/\", exist_ok=True)\nos.makedirs(\"/kaggle/working/output_images/\", exist_ok=True)\n\n# âœ… Load Networks\ndef load_network_stageI(args, device):\n    netG = Generator64(args).to(device)\n    netD = Discriminator64(args).to(device)\n    return netG, netD\n\ndef load_network_stageII(args, device):\n    netG = Generator128(args).to(device)\n    netD = Discriminator128(args).to(device)\n    pre_generator = Generator64(args).to(device)\n    pre_discriminator = Discriminator64(args).to(device)\n    return netG, netD, pre_generator, pre_discriminator\n\ndef load_network_stageIII(args, device):\n    netG = Generator256(args).to(device)\n    netD = Discriminator256(args).to(device)\n    pre_generator = Generator128(args).to(device)\n    pre_discriminator = Discriminator128(args).to(device)\n    return netG, netD, pre_generator, pre_discriminator\n\ndef load_weight(netG, netD, pre_generator=None, pre_discriminator=None):\n    netG.apply(inits_weight)\n    netD.apply(inits_weight)\n\n    if args.NET_G != '':\n        state_dict = torch.load(args.NET_G, map_location=lambda storage, loc: storage)\n        netG.load_state_dict(state_dict)\n        print('Loaded Generator from:', args.NET_G)\n\n    if args.NET_D != '':\n        state_dict = torch.load(args.NET_D, map_location=lambda storage, loc: storage)\n        netD.load_state_dict(state_dict)\n        print('Loaded Discriminator from:', args.NET_D)\n\n    if args.STAGE == 1:\n        return netG, netD\n\n    if args.STAGE in [2, 3]:\n        if args.STAGE1_G != '' and args.STAGE1_D != '':\n            state_dict = torch.load(args.STAGE1_G, map_location=lambda storage, loc: storage)\n            state_dict2 = torch.load(args.STAGE1_D, map_location=lambda storage, loc: storage)\n            pre_generator.load_state_dict(state_dict)\n            pre_discriminator.load_state_dict(state_dict2)\n            pre_generator.eval()\n            pre_discriminator.eval()\n            print('Loaded Stage 1 Generator from:', args.STAGE1_G)\n        else:\n            print(\"Please provide Stage 1 Generator and Discriminator paths.\")\n            return netG, netD\n\n        return netG, netD, pre_generator, pre_discriminator\n\ndef define_optimizers(args, generator, discriminator):\n    optim_gen = torch.optim.Adam(generator.parameters(), lr=args.lr_gen, betas=(args.beta1, args.beta2))\n    optim_dis = torch.optim.Adam(discriminator.parameters(), lr=args.lr_dis, betas=(args.beta1, args.beta2))\n    return optim_gen, optim_dis\n\ndef save_checkpoint(generator, discriminator, optim_gen, optim_dis, epoch, stage, pre_generator=None, pre_discriminator=None):\n    checkpoint = {\n        'epoch': epoch + 1,\n        'generator_state': generator.state_dict(),\n        'discriminator_state': discriminator.state_dict(),\n        'optimizer_gen_state': optim_gen.state_dict(),\n        'optimizer_dis_state': optim_dis.state_dict(),\n        'stage': stage\n    }\n    if stage >= 2 and pre_generator and pre_discriminator:\n        checkpoint['pre_generator_state'] = pre_generator.state_dict()\n        checkpoint['pre_discriminator_state'] = pre_discriminator.state_dict()\n\n    save_path = f\"/kaggle/working/models/ctgan_checkpoint_epoch_{epoch}.pth\"\n    torch.save(checkpoint, save_path)\n    print(f\"âœ… Checkpoint saved at {save_path}\")\n\ndef train(generator, discriminator, optim_gen, optim_dis, train_loader, test_loader,\n          pre_generator=None, pre_discriminator=None, device='cuda', stage=1,\n          save_dir=\"output_images\", start_epoch=0):\n\n    generator.train()\n    discriminator.train()\n    os.makedirs(save_dir, exist_ok=True)\n\n    print(f\"ðŸš€ Starting training from epoch {start_epoch}\")\n\n    for epoch in range(start_epoch, args.epoch):\n        epoch_folder = os.path.join(save_dir, f\"epoch_{epoch+1}\")\n        os.makedirs(epoch_folder, exist_ok=True)\n\n        print(f\"Epoch {epoch+1}/{args.epoch}\")\n\n        for batch_idx, (real_imgs, text_emb) in enumerate(train_loader):\n            real_imgs, text_emb = real_imgs.to(device), text_emb.to(device)\n\n            optim_dis.zero_grad()\n            with torch.no_grad():\n                if stage == 1:\n                    noise = torch.randn((real_imgs.size(0), args.z_dim), device=device)\n                    fake_imgs = generator(noise, text_emb)\n                elif stage == 2:\n                    filtered_imgs = pre_gen_imgs(text_emb, pre_generator, pre_discriminator, device=device)\n                    fake_imgs = generator(filtered_imgs, text_emb)\n                elif stage == 3:\n                    intermediate_imgs = pre_gen_imgs(text_emb, pre_generator, pre_discriminator, device=device)\n                    fake_imgs = generator(intermediate_imgs, text_emb)\n\n            d_out = compute_discriminator_loss(args, discriminator, real_imgs, fake_imgs, text_emb)\n            if len(d_out) == 4:\n                d_loss, errD_real, errD_wrong, errD_fake = d_out\n            else:\n                d_loss, errD_real, errD_fake = d_out\n                errD_wrong = torch.tensor(0.0)\n            d_loss.backward()\n            optim_dis.step()\n\n            optim_gen.zero_grad()\n            if stage == 1:\n                noise = torch.randn((real_imgs.size(0), args.z_dim), device=device)\n                gen_imgs = generator(noise, text_emb)\n            elif stage == 2:\n                gen_imgs = generator(filtered_imgs, text_emb)\n            elif stage == 3:\n                gen_imgs = generator(intermediate_imgs, text_emb)\n\n            g_loss = compute_generator_loss(args, discriminator, gen_imgs, text_emb)\n            g_loss.backward()\n            optim_gen.step()\n\n            batch_img_path = os.path.join(epoch_folder, f\"batch_{batch_idx}.png\")\n            save_image(gen_imgs[:16], batch_img_path, normalize=True)\n\n            if batch_idx % 50 == 0:\n                show_image(batch_img_path)\n\n            if batch_idx % 100 == 0:\n                print(f\"[Epoch {epoch+1}] [Batch {batch_idx}/{len(train_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}\")\n\n        final_epoch_image = os.path.join(epoch_folder, f\"epoch_{epoch+1}_final.png\")\n        save_image(gen_imgs[:16], final_epoch_image, normalize=True)\n        print(f\"âœ… Saved final epoch image: {final_epoch_image}\")\n        show_image(final_epoch_image)\n\n        # Save model if it improves\n        # if FID_score < best_FID:\n        #     torch.save(generator.state_dict(), \"/kaggle/working/models/best_generator.pth\")\n        #     best_FID = FID_score\n        #     print(\"Updated Best Generator Model!\")\n\n        # if IS_score > best_IS:\n        #     torch.save(generator.state_dict(), \"/kaggle/working/models/best_generator.pth\")\n        #     best_IS = IS_score\n        #     print(\"Updated Best Generator Model!\")\n\n        if (epoch + 1) % 50 == 0 or (epoch + 1) == args.epoch:\n            save_checkpoint(generator, discriminator, optim_gen, optim_dis, epoch + 1, stage, pre_generator, pre_discriminator)\n\ndef main():\n    print(\"Initializing Training...\")\n\n    if args.STAGE == 1:\n        generator, discriminator = load_network_stageI(args, device)\n        pre_generator, pre_discriminator = None, None\n    elif args.STAGE == 2:\n        generator, discriminator, pre_generator, pre_discriminator = load_network_stageII(args, device)\n    elif args.STAGE == 3:\n        generator, discriminator, pre_generator, pre_discriminator = load_network_stageIII(args, device)\n    else:\n        raise ValueError(\"Invalid STAGE. Must be 1, 2 or 3.\")\n\n    dataset = ImageDataset(args, cur_img_size=64)\n    train_loader = dataset.train\n    test_loader = dataset.test\n\n    # Load weights for generator and discriminator\n    if args.STAGE == 1:\n        result = load_weight(generator, discriminator)\n        if len(result) == 2:\n            generator, discriminator = result\n        else:\n            raise ValueError(f\"Unexpected return values from load_weight(): {result}\")\n    else:\n        result = load_weight(generator, discriminator, pre_generator, pre_discriminator)\n        if len(result) == 4:\n            generator, discriminator, pre_generator, pre_discriminator = result\n        else:\n            raise ValueError(f\"Unexpected return values from load_weight(): {result}\")\n\n    optim_gen, optim_dis = define_optimizers(args, generator, discriminator)\n\n    # âœ… Resume from checkpoint if available\n    checkpoint_path = \"your_checkpoint_path\"  # update this path\n    start_epoch = 101\n\n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(checkpoint['generator_state'])\n        discriminator.load_state_dict(checkpoint['discriminator_state'])\n        optim_gen.load_state_dict(checkpoint['optimizer_gen_state'])\n        optim_dis.load_state_dict(checkpoint['optimizer_dis_state'])\n        start_epoch = checkpoint['epoch']\n        print(f\"âœ… Resuming from checkpoint at epoch {start_epoch}\")\n\n        # Also load pre_generator and pre_discriminator for stage 2/3\n        if args.STAGE >= 2:\n            pre_generator.load_state_dict(checkpoint['pre_generator_state'])\n            pre_discriminator.load_state_dict(checkpoint['pre_discriminator_state'])\n            pre_generator.eval()\n            pre_discriminator.eval()\n    else:\n        print(\"âš ï¸ Checkpoint not found. Starting from scratch.\")\n\n    target_epoch = args.epoch\n\n    print(f\"Starting training from epoch {start_epoch} to {target_epoch}\")\n    train(generator, discriminator, optim_gen, optim_dis, train_loader, test_loader,\n          pre_generator, pre_discriminator, device=device, stage=args.STAGE,\n          save_dir=\"output_images\", start_epoch=start_epoch)\n\n    print(\"âœ… Training Completed!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ðŸ§ª Setup\nargs = cfg.parse_args()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"ðŸ§ª Testing on device:\", device)\n\ncheckpoint_path = \"your_checkpoint_path\"\noutput_dir = \"your_output_images path\"\nos.makedirs(output_dir, exist_ok=True)\n\ndataset = ImageDataset(args, cur_img_size=64)\ntest_loader = dataset.test\n\n# Load model\nif args.STAGE == 1:\n    generator, _ = load_network_stageI(args, device)\nelse:\n    generator, _, pre_generator, pre_discriminator = load_network_stageII(args, device)\n\n# Load weights from checkpoint\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    generator.load_state_dict(checkpoint['generator_state'])\n    print(f\"âœ… Loaded generator from checkpoint: {checkpoint_path}\")\nelse:\n    raise FileNotFoundError(\"Checkpoint not found.\")\n\ngenerator.eval()\nif args.STAGE == 2:\n    pre_generator.load_state_dict(checkpoint['pre_generator_state'])\n    pre_discriminator.load_state_dict(checkpoint['pre_discriminator_state'])\n    pre_generator.eval()\n    pre_discriminator.eval()\n\n# Inference\ndef generate_images():\n    print(\"ðŸŽ¨ Generating images from test embeddings...\")\n    with torch.no_grad():\n        for idx, (real_img, emb) in enumerate(test_loader):\n            emb = emb.to(device)\n            batch_size = emb.size(0)\n\n            if args.STAGE == 1:\n                noise = torch.randn((batch_size, args.z_dim), device=device)\n                fake_imgs = generator(noise, emb)\n            else:\n                filtered = pre_gen_imgs(emb, pre_generator, pre_discriminator, device=device)\n                fake_imgs = generator(filtered, emb)\n\n            save_path = os.path.join(output_dir, f\"sample_{idx}.png\")\n            save_image(fake_imgs[:16], save_path, normalize=True)\n            print(f\"âœ… Saved test image: {save_path}\")\n            show_image(save_path)\n\n            # if idx >= 4:  # Just generate 5 batches for quick test\n            #     break\n\ndef show_image(path):\n    img = Image.open(path)\n    display.display(img)\n\n# Run test\ngenerate_images()\nprint(\"âœ… Test image generation complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}